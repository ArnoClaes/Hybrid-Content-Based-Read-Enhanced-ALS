{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "SMiUGC2bLYJV"
   },
   "outputs": [],
   "source": [
    "#Data browsing\n",
    "import glob\n",
    "from google.colab import files\n",
    "from google.colab import drive\n",
    "\n",
    "#Data handling\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "#HTML pull\n",
    "import requests\n",
    "from lxml.html import fromstring\n",
    "\n",
    "#Doc2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument #Load in model and TD splits up sentences in lists of words.\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer #Tokenizer\n",
    "\n",
    "import numpy as np\n",
    "import scipy.spatial as sp\n",
    "import logging\n",
    "import  random\n",
    "random.seed(sum([ord(c) for c in \"KNAB\"]))\n",
    "import matplotlib.pyplot as plt\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PsFCJEKe2xwq"
   },
   "source": [
    "#Pull data from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25894,
     "status": "ok",
     "timestamp": 1552406507666,
     "user": {
      "displayName": "Arno Claes",
      "photoUrl": "",
      "userId": "18358942717775719175"
     },
     "user_tz": -60
    },
    "id": "Iy4qDR6_LygY",
    "outputId": "c163d246-ddff-4219-db26-199477d2f9e4"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2530,
     "status": "ok",
     "timestamp": 1551863219499,
     "user": {
      "displayName": "Arno Claes",
      "photoUrl": "",
      "userId": "18358942717775719175"
     },
     "user_tz": -60
    },
    "id": "1TSg8RsPL5iJ",
    "outputId": "c372fcf0-628f-4d0a-a3e1-50d3eab6c6ff"
   },
   "outputs": [],
   "source": [
    "!ls \"/content/drive/My Drive/Knab/Data/CleanData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "E4Kpw7xOpoDE"
   },
   "outputs": [],
   "source": [
    "df_clean_article = pd.read_csv(\"/content/drive/My Drive/Knab/Data/CleanData/clean_article_data.csv\") #import article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1949
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2677,
     "status": "ok",
     "timestamp": 1552406519754,
     "user": {
      "displayName": "Arno Claes",
      "photoUrl": "",
      "userId": "18358942717775719175"
     },
     "user_tz": -60
    },
    "id": "aXWOQERwo5FI",
    "outputId": "12d845ee-3326-48bd-8950-3cba32dfbe4d"
   },
   "outputs": [],
   "source": [
    "df_clean_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P25jFjYf264l"
   },
   "source": [
    "#Start building doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "RQX_p1fK2wsx"
   },
   "outputs": [],
   "source": [
    "#Load in the data\n",
    "data = df_clean_article['TEXT'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oGcATsyz5voW"
   },
   "outputs": [],
   "source": [
    "#Split all the documents into tagged documents\n",
    "#We use regex tokenizer to remove all charachters except letters and numbers \\w+ equals [a-zA-Z0-9_]+\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "split_data = [TaggedDocument(words=tokenizer.tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Z4k2VPab6ZPf"
   },
   "outputs": [],
   "source": [
    "#ADD SUBSAMPLING AND NEGATIVE SAMPLING\n",
    "max_epochs = 600 #number of iterations\n",
    "vec_size = 320 #vector size\n",
    "alpha = 0.025 #learning rate\n",
    "pretrained_emb = \"/content/drive/My Drive/Knab/Algos/wikipedia-320.txt\" #1097047 word vectors\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha = 0.00025,\n",
    "                min_count = 2,\n",
    "                hs = 0,    #1 turns on hierarchical sampling; this is rarely turned on as negative sampling is in general better\n",
    "                negative = 5, #number of negative samples; 5 is a good value\n",
    "                sample = 1e-6, #this is the sub-sampling threshold to downsample frequent words; 1e-5 is usually good for DBOW, and 1e-6 for DMPV\n",
    "                pretrained_emb=pretrained_emb,\n",
    "                dm = 1, #0 = DBOW; 1 = DMPV\n",
    "                dbow_words = 0, #1 turns on updating of word embeddings\n",
    "                window = 9,\n",
    "                dm_concat = 0, #1 = concatenate input word vectors for DMPV; 0 = sum/average input word vectors. \n",
    "                dm_mean = 1,  #1 = average input word vectors; 0 = sum input word vectors.\n",
    "                epochs = max_epochs,\n",
    "                seed = sum([ord(c) for c in \"KNAB\"]),\n",
    "                worker = 4   #should be set to 1 for reproducibility\n",
    "                )\n",
    "  \n",
    "model.build_vocab(split_data)\n",
    "model.train(split_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 816,
     "status": "ok",
     "timestamp": 1550226998767,
     "user": {
      "displayName": "Lennert Aerts",
      "photoUrl": "",
      "userId": "08865342511418318534"
     },
     "user_tz": -60
    },
    "id": "3amL3v8DRAL0",
    "outputId": "dddbd4f4-4362-4f1f-a9a4-8acfa008eabf"
   },
   "outputs": [],
   "source": [
    "article = \"0\"\n",
    "similar_doc = model.docvecs.most_similar(article, topn=5)\n",
    "\n",
    "print(similar_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1550227003658,
     "user": {
      "displayName": "Lennert Aerts",
      "photoUrl": "",
      "userId": "08865342511418318534"
     },
     "user_tz": -60
    },
    "id": "-DF1G3hyRAEd",
    "outputId": "a0d5e0d7-707c-4da6-d354-9f79b0bb38a5"
   },
   "outputs": [],
   "source": [
    "print(df_clean_article['TITLE'][int(article)]+'\\n',\n",
    "      df_clean_article['TITLE'][int(similar_doc[0][0])]+\" \"+str(similar_doc[0][1])+'\\n',\n",
    "      df_clean_article['TITLE'][int(similar_doc[1][0])]+\" \"+str(similar_doc[1][1])+'\\n',\n",
    "      df_clean_article['TITLE'][int(similar_doc[2][0])]+\" \"+str(similar_doc[2][1])+'\\n',\n",
    "      df_clean_article['TITLE'][int(similar_doc[3][0])]+\" \"+str(similar_doc[3][1])+'\\n',\n",
    "      df_clean_article['TITLE'][int(similar_doc[4][0])]+\" \"+str(similar_doc[4][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 968,
     "status": "ok",
     "timestamp": 1550226980459,
     "user": {
      "displayName": "Lennert Aerts",
      "photoUrl": "",
      "userId": "08865342511418318534"
     },
     "user_tz": -60
    },
    "id": "fw6EA-1uf74q",
    "outputId": "216545a0-54e8-4729-f942-a61be6b78366"
   },
   "outputs": [],
   "source": [
    "print(df_clean_article['TITLE'][int(article)]+'\\n',\n",
    "      df_clean_article['TITLE'][int(similar_doc[0][0])]+\" \"+str(similar_doc[0][1])+'\\n',\n",
    "      df_clean_article['TITLE'][int(similar_doc[1][0])]+\" \"+str(similar_doc[1][1])+'\\n',\n",
    "      df_clean_article['TITLE'][int(similar_doc[2][0])]+\" \"+str(similar_doc[2][1])+'\\n',\n",
    "      df_clean_article['TITLE'][int(similar_doc[3][0])]+\" \"+str(similar_doc[3][1])+'\\n',\n",
    "      df_clean_article['TITLE'][int(similar_doc[4][0])]+\" \"+str(similar_doc[4][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YXPXMNj9-fRa"
   },
   "source": [
    "#Creating similarity matrices using some similarity measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1NE9Tvxi9JF6"
   },
   "outputs": [],
   "source": [
    "inferred_matrix = np.zeros([len(split_data),vec_size]) # np.zeros([320])  790 x 320\n",
    "for doc_id in range(len(split_data)):\n",
    "  model.random.seed(sum([ord(c) for c in \"KNAB\"])) #Force the same seed\n",
    "  inferred_vector = model.infer_vector(split_data[doc_id].words)  #retrieve inferred vector for article from the model of size 320\n",
    "  mags = np.linalg.norm(inferred_vector, axis=0)\n",
    "  unit_vecs = inferred_vector / mags   #normalizing the inferred vector\n",
    "  inferred_matrix[doc_id] = unit_vecs  #storing the normalized inferred vector in a matrix\n",
    "sim_matrix = inferred_matrix.dot(np.matrix.transpose(inferred_matrix)) #cosine similarity\n",
    "corrsim_matrix = np.corrcoef(inferred_matrix) #pearson similarity (correlation)\n",
    "\n",
    "url_list = df_clean_article['URL'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4IAfe3q4RaPj"
   },
   "outputs": [],
   "source": [
    "#Save to npz\n",
    "np.savez('/content/drive/My Drive/Knab/Data/CleanData/CB_pearson.npz', corrsim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2SB7V4w0UH-W"
   },
   "outputs": [],
   "source": [
    "#Save\n",
    "df_url= pd.DataFrame(url_list)\n",
    "df_url.to_csv(\"/content/drive/My Drive/Knab/Data/CleanData/url_CB.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PzedmCx_CnO5"
   },
   "source": [
    "#Providing the oppurtunity to add a new article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VAEAeCZVDFPH"
   },
   "outputs": [],
   "source": [
    "new_article = np.array([getpass.getpass('Copy the text of the new article here: ')]) #provide text of new article\n",
    "new_data = [TaggedDocument(words=tokenizer.tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(new_article)]\n",
    "model.random.seed(sum([ord(c) for c in \"KNAB\"])) #Force the same seed\n",
    "new_inferred_vector = model.infer_vector(new_data[0].words)  #retrieve inferred vector for article from the model of size 320\n",
    "new_mags = np.linalg.norm(new_inferred_vector, axis=0)\n",
    "new_unit_vec = new_inferred_vector / new_mags   #normalizing the inferred vector\n",
    "np.vstack((inferred_matrix,new_unit_vec)) #add new article vector to the vector matrix\n",
    "\n",
    "sim_matrix = inferred_matrix.dot(np.matrix.transpose(inferred_matrix)) #cosine similarity\n",
    "corrsim_matrix = np.corrcoef(inferred_matrix) #pearson similarity (correlation)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ItemSimilarity",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
